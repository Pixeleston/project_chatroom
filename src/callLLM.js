// callLLM.js
import fetch from 'node-fetch'
// npm install node-fetch
import { encoding_for_model } from '@dqbd/tiktoken'
import { LLM_CONFIG } from './config.js'
import { TOKEN_CONFIG } from './config.js'
import { OPENAI_API_KEY } from './config.js'

// export function countTokens(text, model = 'gpt-4') {
//   const encoder = encoding_for_model(model)
//   const tokens = encoder.encode(text)
//   return tokens.length
// }

/* ======================================
   model:
   "qwen:32b"
   "gpt-4o-mini"
   ====================================== */

const MODEL_MAP = {
  "qwen:32b": callOllama,
  "yi:9b": callOllama,
  "gpt-4o-mini": callGPT,
  "gpt-4o": callGPT,
  "gpt-5": callGPT5
}


export async function callLLM(model, prompt, Class="[others]") {
  const handler = MODEL_MAP[model]
  if (!handler) {
    console.error(`no model : ${model}.\n Available models : ${OllamaList}\n, ${GPTList}\n`)
    return null
  }
  return handler(model, prompt, Class)
}


// chatGPT-5 ä½¿ç”¨æ•™å­¸ï¼šhttps://platform.openai.com/docs/guides/latest-model?custom-tools-mode=responses&reasoning-effort-mode=chat
export async function callGPT5(model, prompt, Class) {
  // 1ï¸âƒ£ è¨ˆç®— input tokens

  
  // const encoder = encoding_for_model(model)
  // const promptTokens = encoder.encode(prompt).length
//  console.log("!!!!!!!!!!!!!!!!!!!!!!!!!???????????????????????????")

  // 2ï¸âƒ£ ç™¼é€ API è«‹æ±‚
  let response
  const body = {
    model,
    messages: [{ role: 'user', content: prompt }],
    reasoning_effort: "minimal"
  }

  // if (stop) {
  //   body.stop = stop
  // }

  response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(body),
  })

  const data = await response.json()

  console.log(data)
  console.error(data.error)

  if (!response.ok) {
    console.error(`âŒ API å›æ‡‰éŒ¯èª¤ï¼š${response.status}`)
    return null
  }

  

  const reply = data.choices?.[0]?.message?.content?.trim() ?? '(ç„¡å›æ‡‰)'
  const usage = data.usage || {}

  // if(TOKEN_CONFIG.consoleLogToken){
  //   const completionTokens = usage.completion_tokens ?? 0
  //   const totalTokens = usage.total_tokens ?? (promptTokens + completionTokens)

  //   // 3ï¸âƒ£ é¡¯ç¤º token è¨ˆç®—
  //   console.log("=============== Tokens Usage ===============")
  //   console.log("é¡åˆ¥ï¼š" + Class)
  //   console.log(`ğŸ§® Token è¨ˆç®—ï¼š
  //   - Input tokens: ${promptTokens}
  //   - Output tokens: ${completionTokens}
  //   - Total tokens: ${totalTokens}
  //   `)
  //   console.log("============================================")
  // }

/*
  const completionTokens = usage.completion_tokens ?? 0
  const totalTokens = usage.total_tokens ?? (promptTokens + completionTokens)

  // 3ï¸âƒ£ é¡¯ç¤º token è¨ˆç®—
  console.log("=============== Tokens Usage ===============")
  console.log(`ğŸ§® Token è¨ˆç®—ï¼š
  - Input tokens: ${promptTokens}
  - Output tokens: ${completionTokens}
  - Total tokens: ${totalTokens}
  `)
  console.log("============================================")
*/

  return reply
}

export async function callGPT(model, prompt, Class) {
  // 1ï¸âƒ£ è¨ˆç®— input tokens

  
  const encoder = encoding_for_model(model)
  const promptTokens = encoder.encode(prompt).length
  

  // 2ï¸âƒ£ ç™¼é€ API è«‹æ±‚
  let response
  const body = {
    model,
    messages: [{ role: 'user', content: prompt }],
    max_tokens: LLM_CONFIG.maxOutputTokens,
    stream: false,
    store: true,
  }

  // if (stop) {
  //   body.stop = stop
  // }

  response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(body),
  })

  if (!response.ok) {
    console.error(`âŒ API å›æ‡‰éŒ¯èª¤ï¼š${response.status}`)
    return null
  }

  const data = await response.json()
  const reply = data.choices?.[0]?.message?.content?.trim() ?? '(ç„¡å›æ‡‰)'
  const usage = data.usage || {}

  // if(TOKEN_CONFIG.consoleLogToken){
  if(Class === "[/api/evaluate]"){
    const completionTokens = usage.completion_tokens ?? 0
    const totalTokens = usage.total_tokens ?? (promptTokens + completionTokens)

    // 3ï¸âƒ£ é¡¯ç¤º token è¨ˆç®—
    console.log("=============== Tokens Usage ===============")
    console.log("é¡åˆ¥ï¼š" + Class)
    console.log(`ğŸ§® Token è¨ˆç®—ï¼š
    - Input tokens: ${promptTokens}
    - Output tokens: ${completionTokens}
    - Total tokens: ${totalTokens}
    `)
    console.log("============================================")
  }

/*
  const completionTokens = usage.completion_tokens ?? 0
  const totalTokens = usage.total_tokens ?? (promptTokens + completionTokens)

  // 3ï¸âƒ£ é¡¯ç¤º token è¨ˆç®—
  console.log("=============== Tokens Usage ===============")
  console.log(`ğŸ§® Token è¨ˆç®—ï¼š
  - Input tokens: ${promptTokens}
  - Output tokens: ${completionTokens}
  - Total tokens: ${totalTokens}
  `)
  console.log("============================================")
*/

  return reply
}

async function callOllama(model, prompt, stop=null) {
  // const tokenCount = countTokens(prompt, "gpt-4o")

  // console.log("=============== Tokens Usage ===============")
  // console.log(`ğŸ§® Input token count: ${tokenCount}`)
  // console.log("============================================")

  const url = "http://127.0.0.1:11434/api/generate"
  const body = {
    model: "qwen:32b",
    prompt,
    stream: false
  }

  try {
    const res = await fetch(url, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(body),
    })

    if (!res.ok) {
      console.error(`LLM response not ok: ${res.status}`)
      return "(éŒ¯èª¤ï¼šLLM å›æ‡‰ç•°å¸¸)"
    }

    const result = await res.json()
    return result.response?.trim() || "(éŒ¯èª¤ï¼šLLM æ²’æœ‰å›æ‡‰)"
  } catch (err) {
    console.error("LLM fetch failed:", err.message)
    return "(éŒ¯èª¤ï¼šç„¡æ³•å‘¼å« LLM)"
  }
}